---
title: "IA Generativa não é o que você imagina"
draft: false
ShowToc: true
---

![image](https://static-blog.onlyoffice.com/wp-content/uploads/2023/06/28125923/What-is-generative-AI_-an-easy-guide.png)

O sujeito digita “ChatGPT” no Google, entra na plataforma da OpenAI, faz uma pergunta e torce para obter a resposta correta. Se isso não te assusta, precisamos conversar.

Em junho de 2017, foi publicado um artigo com um título enigmático, “Attention Is All You Need”. O paper tratava de uma nova arquitetura de rede neural focada na tradução de palavras. O exemplo no artigo é a tradução do inglês para o alemão. A arquitetura da rede neural foi chamada de transformers, em oposição ao modelo vigente, chamado de sequencial, onde cada palavra era fielmente traduzida na ordem em que apareciam. A ideia era que na tarefa de tradução, ao invés de focar em cada trecho da frase, elementos “irrelevantes”, como, digamos, artigos, seriam deixados de lado, e apenas as palavras importantes seriam consideradas, isso foi chamado de “atenção”.

Na prática, nossa rede neural recebe mais dados e, por consequência, mais contexto para que possa “adivinhar” qual é a próxima palavra e a sua relevância com base no que viu antes. Além da tradução de idiomas, podemos usar esse mecanismo de atenção para produzir conteúdo semelhante, como, por exemplo, uma explicação sobre cálculo diferencial e integral no estilo de escrita usado por Shakespeare.

Quando nosso pedido não é atendido corretamente, chamamos isso de alucinação. É quando, por dados insuficientes ou interpretação incorreta do modelo, erra a resposta para nossa pergunta. Nesse caso por que esperar que a sua resposta a alguma pergunta específica seja respondida sem que antes haja um contexto para um problema? Você realmente confia decisões importantes na medicina e engenharia a um modelo que pode alucinar sobre sua resposta, dado que está supondo qual seja a próxima palavra relevante? As IAs não podem substituir humanos (pelo menos ainda), mas podem potencializar nossas decisões e otimizar nosso tempo quando fornecemos contextos e compreendemos o modus operandi delas.

# Referências
Transformers, explained: Understand the model behind GPT, BERT, and T5

Attention Is All You Need — Paper

Let’s build GPT: from scratch — Andrej Karpathy

Let’s embark on the journey of constructing a GPT — Kaggle
